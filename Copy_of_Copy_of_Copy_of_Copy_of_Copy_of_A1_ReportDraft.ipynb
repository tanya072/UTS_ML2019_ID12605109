{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Copy of Copy of Copy of Copy of Copy of A1_ReportDraft.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanya072/UTS_ML2019_ID12605109/blob/master/Copy_of_Copy_of_Copy_of_Copy_of_Copy_of_A1_ReportDraft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGUbBKkyGIRr",
        "colab_type": "text"
      },
      "source": [
        "# Draft and Experiment Area"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCOuRmsaGIRu",
        "colab_type": "text"
      },
      "source": [
        "1. First impression\n",
        "    * What is my chosen paper to read?\n",
        "    * What type of the main contribution the paper has made?\n",
        "        - A theory or proposition (revealing something, from unknown to known)\n",
        "        - A method or algorithm (inventing a technique, from undoable to doable)\n",
        "\n",
        "    * _Before_ reading the main body of the paper, write down your first impression  obtained from its abstract and short introduction.\n",
        "    * Why does the paper attract you, such as, How it surprised you? Why do you think it addresses an important topic that will be helpful in your future study of machine learning?\n",
        "    \n",
        "2. Read the paper abstract and introduction, list here all the notions that you don't know the precise meaning. If you think you have completed your list,  compare the list with people around you who have chosen the same or a similar paper.\n",
        "\n",
        "3. (During the next 7 days) Re-consider the central problem of the paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUKXUVJd9J07",
        "colab_type": "text"
      },
      "source": [
        "It is argued that the role of machine learning and neural networks as one of its techniques is crucial when it comes to image processing and pattern recognition.\n",
        "\n",
        "The underlying idea of the article: \"\"better pattern recognition systems can be built by relying more on automatic learning and less on hand designed heuristics. Using character recognition as a case study we show that hand crafted feature extraction can be advantageously replaced by carefully designed learning machines that operate directly on pixel images.\"\"(to paraphrase)\n",
        "\n",
        "The article is a comprehensive overview of Graph Transformer Networks, machine learning algorithm that uses Gradient-Based methods for online handwriting recognition and based on Convolutional Neural Network character recognisers as one of the deep learning techniques. Graph Transformer Network is designed to address the real-life application such as reading bank check and is deployed commercially with capacity to read a several million checks per day.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBBDF7PwGIRv",
        "colab_type": "text"
      },
      "source": [
        "# Review Report on \"Gradient-Based Learning Applied to Document Recognition\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk9EQEMMGIRw",
        "colab_type": "text"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX1BuK0KDmzC",
        "colab_type": "text"
      },
      "source": [
        "In the article \"Gradient-Based Learning Applied to Document Recognition\" Le Cun et al. (1998) claim that the role of machine learning and neural networks as one of its techniques is crucial when it comes to image processing and pattern recognition. However, until relatively recently, most pattern recognition models were relying heavily on hand-crafted algorithms more than on automatic learning. Hand-crafted algorithm is a feature extractor that locates characteristic regions or features of an image that is designed beforehand by human experts to extract a certain set of features, where they are then represented by low-dimensional vectors and can be distinguished and compared with others. Even though the feature-transformation algorithms are robust to scale variation and object rotation, this robustness comes at a high computational cost. The other issue related to feature extraction is that accuracy of the model is highly depended on what set of features the designers come up with and how these features are relevant to the images in given dataset. Furthermore, the set of features must be redone every time when a new problem arises.\n",
        "\n",
        "With emerging of new powerful machine-learning techniques, availability of low-cost machines and large datasets in early 1990s, the need for new feature extraction methods triggered an idea of developing deep learning algorithms that would obtain a set of features learned directly from observation of the input images. The research described by Le Cun et al. (1998) demonstrates great dedication to this idea and focuses on designing a new learning paradigm  that would reduce the need of the hand-crafted feature extractors, manual labelling and manual parameter tuning in document recognition systems. \n",
        "\n",
        "Le Cun et al. (1998) present a comprehensive overview of Graph Transformer Networks, machine learning algorithm that uses Gradient-Based learning method to minimise a loss function for  a given set of training data samples and their corresponding targets in  handwriting recognition, and is based on Convolutional Neural Network character recognisers as one of the deep learning techniques. Graph Transformer Network is designed to address the real-life application such as reading bank check and is deployed commercially with capacity to read a several million checks per day.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3r4Nr-QGIRx",
        "colab_type": "text"
      },
      "source": [
        "## Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWJaJcgnGIRy",
        "colab_type": "text"
      },
      "source": [
        "The research comprises of a few sections where they are dedicated to following topics and discussions such as considering the task of handwritten character recognition, introduction of convolutional neural network such as LeNet-5 model and its architecture, comparing different learning techniques on a benchmark, introduction the concept of trainable Graph Transformer Network (GTN), describing discriminative and non-discriminative gradient-based techniques for training recogniser without manual segmentation and labelling, introduction of Space Displacement Neural Network without the need for segmentation heuristic, implementation of GTN in online handwriting recognition based on Convolutional Neural Network (CNN),  and finally, description of commercial GTN-based model for reading handwritten and printed bank checks. \n",
        "\n",
        "LeCun et al. (1998) explicitly divides handwriting recognition task on two conceptually different approaches: one is based on isolated or single character recognition, known as segmentation, and have been intensively studied in the literature by Srihari (1992) and other authors; the other one is to read the characters at the word level without manual segmentation and labelling. The reason for emerging handwriting recognition methods that would allow to train the recogniser at the word level rather than training pre-segmented characters, was difficulties that were related to separation out the characters from their neighbours in the sentence or the line of characters. The classic solution of this problem is called Heuristic Over-Segmentation (HOS). The technique is designed so that it generates a large number of potential cuts between characters using image processing techniques and selects the best combination of cuts based on scores assigned to each candidate character by recogniser. The main advantage of HOS is that it takes less time to make a decision about the segmentation due to a large number of possible variations of cuts. However, the accuracy of the method depends on the quality of cuts generated by segmenter and the ability of the recogniser to distinguish correctly segmented characters from pieces of characters, multiple or incorrectly segmented characters. This task presents the major challenge because the labelling database of incorrectly segmented characters can only be performed manually. It is tedious, costly and difficult to perform labelling uniformly due to great variations in naturally written sequences of characters.\n",
        "\n",
        "To address the issue related to segmentation, two solution are represented by LeCun et al. (1998). First one is based on training the model at the level of whole word rather than the character and introduces the idea of GTN. GTN is a generalisation of multilayer network where the information is represented as graphs instead of fixed-sized vectors. The second solution is designed to reject the segmentation altogether. Both solutions are using Gradient-Based Learning methods to minimise an overall loss function and include CNN LeNet-5 as a recogniser. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_VBB0U3GIRz",
        "colab_type": "text"
      },
      "source": [
        "## Innovation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRuHhiUkGIR0",
        "colab_type": "text"
      },
      "source": [
        "Back to late 90s the pattern recognition systems comprised of multiple modules that included field locator, segmenter, recogniser and post-processor. Normally, each module was trained and optimised separately. To maximise the overall performance the system should have been assembled and manually adjusted with respective to a subset of all parameters. Innovative approach illustrated in this paper is to train the recognition system globally finding the global minimum of loss function that would satisfy all parameters in the system using Gradient-Based Learning. \n",
        "Three networks for handwritten recognition are proposed by this research:\n",
        "\n",
        "   a)\thandwriting recognition Graph Transformer Network (GTN) based on heuristic over-segmentation (HOS) where segmentation transformer performs the segmentation on the word level followed by LeNet-5 recogniser which recognises each segment and outputs a recognition graph, followed by a couple other transformers that finally extract the best interpretation from the recognition graph.\n",
        "  \n",
        "   b)\thandwriting recognition GTN based on Space Displacement Neural Network (SDNN) which first module is SDNN transformer and replaces convolutional recogniser. Its main function is to scan every possible location of the input image and spot every well-centered character. \n",
        "    \n",
        "   c)\tGTN Check Reading system comprises of field locator and segmenter followed by recognition transformer LeNet-5 that classifies, and finally a couple transformers that select the path with lowest penalty that corresponds to the best interpretation. \n",
        "    \n",
        "All modules in the system output graphs whose arcs carry numerical information about transformations occurred in the given module. Some parameters of the systems can be adjusted manually, however the weights of neural net recogniser must be learned. GTN learning algorithms utilise the gradient based minimisation methods as a general principle of learning techniques in complex dynamic architectures. They use GTN as one of the modules in order to reduce the need for hand-crafted heuristic, manual labelling and parameter adjustment.  \n",
        "\n",
        "The systems include CNN LeNet-5, the high-level convolutional network, as a character recogniser which plays the role of automated feature extractor and pretrained on database of pre-segmented and labelled characters MNIST dataset that was modified from NIST’s database. MNIST dataset contains 60,000 training and 10,000 test images of 28x28 pixels written characters. LeNet-5 was trained 20 epochs through, with different number of training samples: 15,000, 30,000 and 60,000. Results explicitly show that increased number of samples improve the accuracy of classification. To extend the size of original dataset, the researches apply augmentation technique to the dataset to artificially generate more samples up to 540000 with randomly selected parameters such as horizontal and vertical translations, scaling, squeezing, shearing. As result of data augmenation, the test error improved from 0.95% on original data to 0.8% on distorted data. \n",
        "\n",
        "In addition, two models HOS and SDNN participate in on-line handwriting recognition experiment where they undergo a number of tests with different combination of parameters and with/no global training. The best performance is shown by HOS model with 1.4% error rate (LeCun 1998, p. 37) with global training that is trained on limited 25,000-word dictionary and demonstrates that lower error level can be achieved by reducing the dictionary size. The results illustrate also that pre-processing the whole word is better than first segmenting it and then pre-processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuxuQ2IDGIR1",
        "colab_type": "text"
      },
      "source": [
        "## Technical quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hery5SDyGIR2",
        "colab_type": "text"
      },
      "source": [
        "The technical development of the research is high quality. The authors apply Gradient-Based Learning methods to integrate graph-based statistical models such as GTN with Convolutional Neural Networks (CNN). There are a few performance evaluations based on error rates of performed tests. Firstly, the researchers discuss about different classification models and how they perform on test data with and without applying artificially distorted training data in some cases. All models are trained on MNIST dataset with 60,000 training samples. The methods show various results ranging from 12% error rate for Linear model and 0.7% error rate for Boosted LeNet-4 (LeCun 1998, p.12). Even though, there is no need to do data augmentation due to the large size of the dataset, enlarging the initial dataset improved the accuracy of every model that it was applied to, and is a good example of experimentation practice.\n",
        "\n",
        "Secondly, comparing HOS and SDNN during on-line handwriting recognition experiment are based on sets of different parameters and supposed to measure improvements obtained from global training of the neural network with error rates of the training performed on the character-level. The effectiveness of these tests is arguable because they have quite different parameters, for instance, first set contains 100,000 hand printed characters which are extended in number with distorted data and are four type of characters: lower and upper case, digits and punctuation. The second and third sets are performed for recognition of 881 words. In the figure that illustrates the results and compare two methods, it is not clear what bars on the chart represent and where these results were extracted from. There is not enough explanation in the text either. \n",
        "\n",
        "All the test and experiments described in the study can be easily replicated due to open access to databases and specifications of classification algorithms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAE1GgUNGIR3",
        "colab_type": "text"
      },
      "source": [
        "## Application and X-factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pOCBlYGGIR4",
        "colab_type": "text"
      },
      "source": [
        "I find the proposal in the paper promising. ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RumR0SuNGIR5",
        "colab_type": "text"
      },
      "source": [
        "## Presentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g747TCkOGIR5",
        "colab_type": "text"
      },
      "source": [
        "The overall strucutre is clear. I found reading is easy / difficult. The paper could have been more attractive if the authors had organised ... / provided ... "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTafRFRYGIR6",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "[SHA48][1]: Author, Title, Info\n",
        "\n",
        "[1]:https://google.com"
      ]
    }
  ]
}